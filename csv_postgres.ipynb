{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#import io\n",
    "import os\n",
    "import psycopg2\n",
    "#import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from sqlalchemy.types import String , Integer, DateTime, DECIMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:vgnnr120e@localhost:5432/PaperChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to copy csv from google cloud to local storage\n",
    "# gsutil cp -r gs://paperchain-test-bucket/assignment-data D:\\Users\\Ayshahvez\\test\n",
    "\n",
    "#directory\n",
    "CSV_DIR_token_transfers = 'D:\\\\Users\\\\Ayshahvez\\\\test\\\\assignment-data\\\\token-transfers\\\\'\n",
    "\n",
    "CSV_DIR_tokens = 'D:\\\\Users\\\\Ayshahvez\\\\test\\\\assignment-data\\\\tokens\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split csv objects into into groups of batches\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the CSV files in the directory\n",
    "all_files_token_transfers = os.listdir(CSV_DIR_token_transfers)\n",
    "all_files_tokens = os.listdir(CSV_DIR_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the list of files. Let's go with 50 files per chunk, can be changed\n",
    "chunked_file_list_token_transfers = chunks(all_files_token_transfers, 50)\n",
    "chunked_file_list_tokens = chunks(all_files_tokens, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.raw_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Users\\\\Ayshahvez\\\\test\\\\assignment-data\\\\tokens\\\\'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_DIR_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate the chunks and aggregate the files in each chunk into a single\n",
    "# in-memory file\n",
    "for  chunk in chunked_file_list_tokens:\n",
    "    #index=0\n",
    "    #print (chunk)\n",
    "    csv_data_append = pd.DataFrame(columns=['address','symbol','name','decimals','total_supply'])\n",
    "    for file in chunk:\n",
    "        #index = index+1\n",
    "        file_in_chuck = (CSV_DIR_tokens + file)\n",
    "        #print (file)\n",
    "        #print (index,file)\n",
    "        csv_data = pd.read_csv(file_in_chuck)\n",
    "        #csv_data_append = csv_data.concat(csv_data,)\n",
    "        csv_data_append = pd.concat([csv_data, csv_data_append], axis=0).reset_index(drop=True) # pd.concat(csv_data,csv_data_append)\n",
    "        #end of combining chunk\n",
    "       # if index == 3:\n",
    "        # break\n",
    "    \n",
    "    csv_data_append.to_sql('Tokens', engine, if_exists='append', index=False, dtype={\"address\": String(), \"symbol\": String(), \"name\": String(), \"decimals\": String(), \"total_supply\": String()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate the chunks and aggregate the files in each chunk into a single\n",
    "# in-memory file\n",
    "\n",
    "for  chunk in chunked_file_list_token_transfers:\n",
    "    #index=0\n",
    "    #print (chunk)\n",
    "    csv_data_append = pd.DataFrame(columns=['token_address','from_address','to_address','value','transaction_hash','log_index','block_timestamp','block_number','block_hash'])\n",
    "    for file in chunk:\n",
    "        #index = index+1\n",
    "        file_in_chuck = (CSV_DIR_token_transfers + file)\n",
    "        #print (file)\n",
    "        #print (index,file)\n",
    "        csv_data = pd.read_csv(file_in_chuck)\n",
    "        #csv_data_append = csv_data.concat(csv_data,)\n",
    "        csv_data_append = pd.concat([csv_data, csv_data_append], axis=0).reset_index(drop=True) # pd.concat(csv_data,csv_data_append)\n",
    "        #end of combining chunk\n",
    "       # if index == 3:\n",
    "        # break\n",
    "    \n",
    "    csv_data_append.to_sql('Token_Transfers', engine, if_exists='append', index=False, dtype={\"token_address\": String(), \"from_address\": String(), \"miles_walked\": String(), \"to_address\": String(), \"value\": String(), \"transaction_hash\": String(), \"log_index\": Integer(), \"block_timestamp\":DateTime(), \"block_number\":Integer(),\"block_hash\":String()\n",
    "})\n",
    "    #if index == 3:\n",
    "    # break\n",
    "\n",
    "       # csv_data \n",
    "        #csv_data = csv_data.compute()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3337865716cf622e8229452c21aff9f6f13505bec10c4cfd12eff80614298760"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
